{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mipSoOVlavkb"
   },
   "source": [
    "# ÉCOLE IVADO/MILA EN APPRENTISSAGE PROFOND\n",
    "# SESSION D' AUTOMNE 2018 \n",
    "# Tutoriel : Données Catégorielles (MLP)\n",
    "\n",
    "## Auteurs: \n",
    "\n",
    "Arsène Fansi Tchango <arsene.fansi.tchango@rd.mila.quebec>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLHwvggEZERd"
   },
   "source": [
    "## Préface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKNGtQkkohiM"
   },
   "source": [
    "Ce tutoriel a pour but d'initier les participants aux aspects pratiques du Deep Learning  à travers la réalisation d'un projet simple de bout en bout. Dans le cadre de cet exercice, nous utiliserons le framework de développement <a href=\"https://pytorch.org/\"> `PyTorch`</a>. Celui-ci a été choisie pour sa souplesse et sa flexibilité qui ont pour effet bénéfique de rendre la courbe d'apprentissage du framework facile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOD70vdvvtin"
   },
   "source": [
    "# Initialisation\n",
    "\n",
    "Avant de commencer, nous devons nous assurer d'installer les librairies nécessaires pour le tutoriel à l'aide de `pip`.  Pour se faire, exécutez la cellufle suivante en la sélectionnant et en cliquant `shift`+`Enter`. Ceci peut prendre quelques minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51918,
     "status": "ok",
     "timestamp": 1539775182153,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "c5AlBPjnvzNh",
    "outputId": "25af9e31-6ed4-41a9-a3de-c4d933cbb426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 519.5MB 28kB/s \n",
      "tcmalloc: large alloc 1073750016 bytes == 0x5977a000 @  0x7f72265892a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
      "\u001b[?25hCollecting torchvision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 21.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow) (0.46)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2018.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.2.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
      "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-0.4.1 torchvision-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision Pillow matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djF9gjzLwsDB"
   },
   "source": [
    "Afin de vous assurer que l'installation s'est bien faite, importez toutes les libraries et modules dont nous nous servirons pour ce tutoriel en exécutant la prochaine cellule:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6776,
     "status": "ok",
     "timestamp": 1539775410716,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "w9LnNnxBw0wC",
    "outputId": "d4f1b1a5-583e-4c19-d1c4-825187f8c482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  0.4.1\n",
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "\n",
    "print(\"Torch version: \", torch.__version__)\n",
    "print(\"GPU Available: {}\".format(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKzgFV9Favkt"
   },
   "source": [
    "## PyTorch en bref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vrus_-F0avkt"
   },
   "source": [
    "*PyTorch* est une librairie Python qui fournit deux fonctionnalités de haut niveau:\n",
    "<ul>\n",
    "<li> Opérations sur des tenseurs (comme NumPy) avec support GPU </li>\n",
    "<li> Réseaux de neurones profonds construits sur un système de <b> différentiation automatique</b> appelé  <b> <a href=\"http://pytorch.org/docs/master/autograd.html\">Autograd:  `torch.autograd`</a> </b>.</li>\n",
    "</ul>\n",
    "<br/>\n",
    "Voici les documentations utiles relatives à ce sujet: \n",
    "<ul>\n",
    "<li>  La documentation principale sur PyTorch: <a href=\"http://pytorch.org/docs/master/torch.html\"> `Docs - PyTorch` .</a> </li>\n",
    "<li>  La documentation principale  sur les réseaux de neurones: <a href=\"http://pytorch.org/docs/master/nn.html\">`torch.nn`</a>. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7oPl95cswjd"
   },
   "source": [
    "En plus d'offrir des facilités pour définir et manipuler les réseaux de neuronnes, PyTorch offre plusieurs utilitaires pour le traitement des données. \n",
    "<br/> \n",
    "Un de ces utilitaires est la classe <b><a href=\"http://pytorch.org/docs/master/data.html#\"> `torch.utils.data.Dataset`</a> </b> et ses sous-classes (`torch.utils.data.TensorDataset`, `torch.utils.data.Subset`, `etc...`) qui offrent une interface facile d'utilisation pour manipuler un jeu de données.\n",
    "<br/>\n",
    "Pour plus d'informations, veuillez vous reportez aux urls suivantes: \n",
    "<ul>\n",
    "<li>Les jeux de données en PyTorch: <a href=\"http://pytorch.org/docs/master/data.html\"> `Datasets - PyTorch` .</a>  </li>\n",
    "<li>Un tutoriel pour le chargement des données: <a href=\"http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\"> `Data Loading Tutorial - PyTorch` .</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qm122vNmq92L"
   },
   "source": [
    "## Elements nécessaires pour un projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqvhR0ebavmE"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Pour mener à bien un projet en deep learning, on a besoin de:\n",
    "<ul>\n",
    "<li>Une <b> tâche à résoudre</b> ainsi que des <b>données</b> pour la supporter </li>\n",
    "<li>Un <b>modèle</b> (réseau de neurones) à entraîner </li>\n",
    "<li>Une <b>fonction de coût</b> à optimiser </li>\n",
    "<li>Un <b>optimiseur</b> qui ajustera les paramètres (poids) du réseau de neurones</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8_pfpu2f6AO"
   },
   "source": [
    "# DÉFINITION DE LA TÂCHE:  Prédiction de la survie suite à un naufrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5piZxYUhSzq"
   },
   "source": [
    "Notre objectif est de <b>prédire si un passager a survécu ou non suite au naufrage du Titanic</b> en se basant sur des données obtenues sur les passagers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4GuYNDFavlU"
   },
   "source": [
    "## Le dataset Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiOJx2ytavlU"
   },
   "source": [
    "Le jeu de données Titanic peut être téléchargé à l'adresse suivante:  https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true.<br/>\n",
    "Cet ensemble de données fournit des informations sur le sort de 1309 passagers du premier voyage fatal du paquebot \"Titanic\", résumées par <br/>\n",
    "statut économique (classe), sexe, âge, les informations familiales et survie. Cet ensembles de données est  également celui utilisé par le <br/>\n",
    "concours Kaggle et permet ainsi de réduire  la barrière à l'entrée pour les utilisateurs débutants en apprentissage machine.\n",
    "\n",
    "\n",
    "Nous utiliserons la librarire <a href=\"https://pandas.pydata.org/\"> <b> Pandas </b></a>   pour charger en mémoire le jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1539775895941,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "bX_RSiffavlW",
    "outputId": "310823e7-36c6-466b-8c2a-ba96fdcb20de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                                             name     sex  \\\n",
       "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
       "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
       "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
       "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
       "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
       "\n",
       "       age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
       "0  29.0000      0      0   24160  211.3375       B5        S    2    NaN   \n",
       "1   0.9167      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
       "2   2.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "3  30.0000      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
       "4  25.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  \n",
       "2  Montreal, PQ / Chesterville, ON  \n",
       "3  Montreal, PQ / Chesterville, ON  \n",
       "4  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = pd.read_csv(\n",
    "    'https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true', \n",
    "    sep='\\t', \n",
    "    index_col=None, \n",
    "    na_values=['NA']\n",
    ")\n",
    "\n",
    "# Un appeçu des données\n",
    "\n",
    "titanic_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yj88WmCmavlf"
   },
   "source": [
    "**La signification des différentes colonnes (features) est la suivante**:\n",
    "\n",
    "<ol>\n",
    "\n",
    "  <li> <b>pclass</b>: Classe du Passager (1 = première; 2 = seconde; 3 = troisième) </li>\n",
    "  <li> <b>survived</b>: Survie (0 = non; 1 = oui) </li>\n",
    "  <li> <b>name</b>: Nom </li>\n",
    "  <li> <b>sex</b>: Sexe </li>\n",
    "  <li> <b>age</b>: Âge </li>\n",
    "  <li> <b>sibsp</b>: Nombre de frères, sœurs, ou conjoints à bord </li>\n",
    "  <li> <b>parch</b>: Nombre de parents ou enfants à bord </li>\n",
    "  <li> <b>ticket</b>: Numéro de ticket </li>\n",
    "  <li> <b>fare</b>: Tarif passager </li>\n",
    "  <li> <b>cabin</b>: Numéro de cabine </li>\n",
    "  <li> <b>embarked</b>: Port d'embarquement (C = Cherbourg; Q = Queenstown; S = Southampton) </li>\n",
    "  <li> <b>boat</b>: Canot de sauvetage (si le passager a survécu) </li>\n",
    "  <li> <b>body</b>: Numéro de corps (si le passager n'a pas survécu et que son corps a été retrouvé) </li>\n",
    "  <li> <b>home.dest</b>: la destination du passager </li>\n",
    " </ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2ed5fozqjce"
   },
   "source": [
    "## Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__vcZhPnavlg"
   },
   "source": [
    "Certaines données sont **moins importantes** que d'autres, par exemple:\n",
    "<ol>\n",
    "  <li> <b>name</b>: Nom </li>\n",
    "  <li> <b>ticket</b>: Numéro de ticket </li>\n",
    "  <li> <b>cabin</b>: Numéro de cabine </li>\n",
    "  <li> <b>home.dest</b>: la destination du passager </li>\n",
    " </ol>\n",
    " \n",
    "\n",
    "\n",
    "D'autres données sont **fortement** correlées à notre tâche, et les inclure reviendrait à **tricher**:\n",
    "<ol>\n",
    "  <li> <b>boat</b>: Canot de sauvetage (si le passager a survécu) </li>\n",
    "  <li> <b>body</b>: Numéro de corps (si le passager n'a pas survécu et que son corps a été retrouvé) </li>\n",
    " </ol>\n",
    " \n",
    "\n",
    " \n",
    " \n",
    "Par ailleurs, nous remarquons la présence des features de type **variables catégorielles**, il faut les transformer en données numériques avec de l'<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">**encodage one-hot**</a>:\n",
    " <ol>\n",
    "  <li> <b>pclass</b>: Classe du Passager </li>\n",
    "  <li> <b>sex</b>: Sexe </li>\n",
    "  <li> <b>embarked</b>: Port d'embarquement </li>\n",
    " </ol>  \n",
    "  <br/>\n",
    " **Remarque:** La colonne <b>pclass</b> est apparemment numérique. Toutefois, cette représentation induit implicitement un biais. En effet, numériquement parlant, nous avons 1 (première classe) < 2 (secode classe) < 3 (troisième classe). On aurait pu multiplier cette colonne par `-1` pour réfléter notre société mais nous avons opté de rester neutre. \n",
    " <br/>\n",
    " <br/>\n",
    " \n",
    " Le dataset pré-processé peut être téléchargé à l'adresse suivante:  https://github.com/afansi/winterschool18/blob/master/titanic_prepocess.csv?raw=true. \n",
    " La signification des variables est la suivante:\n",
    "\n",
    "<ol>\n",
    "  <li> <b>survived</b>: Survie (0 = non; 1 = oui) </li>\n",
    "  <li> <b>pclass_1</b>: (1 si passager en première classe; 0 sinon) </li>\n",
    "  <li> <b>pclass_2</b>: (1 si passager en seconde classe; 0 sinon) </li>\n",
    "  <li> <b>pclass_3</b>: (1 si passager en troisième classe; 0 sinon) </li>\n",
    "  <li> <b>sex_female</b>: (1 si passager est une femme; 0 sinon) </li>\n",
    "  <li> <b>sex_male</b>: (1 si passager est un homme; 0 sinon) </li>\n",
    "  <li> <b>age</b>: Âge </li>\n",
    "  <li> <b>sibsp</b>: Nombre de frères, sœurs, ou conjoints à bord </li>\n",
    "  <li> <b>parch</b>: Nombre de parents ou enfants à bord </li>\n",
    "  <li> <b>fare</b>: Tarif passager </li>\n",
    "  <li> <b>embarked_C</b>: (1 si Port d'embarquement = Cherbourg (C); 0 sinon) </li> \n",
    "  <li> <b>embarked_Q</b>: (1 si Port d'embarquement = Queenstown (Q); 0 sinon) </li> \n",
    "  <li> <b>embarked_S</b>: (1 si Port d'embarquement = Southampton (S); 0 sinon)</li> \n",
    " </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1539776190414,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "JJ0--SDpavlg",
    "outputId": "fe08ba4d-d829-4835-b91f-f87d71e3f6e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass_1</th>\n",
       "      <th>pclass_2</th>\n",
       "      <th>pclass_3</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass_1  pclass_2  pclass_3  sex_female  sex_male      age  \\\n",
       "0         1         1         0         0           1         0  29.0000   \n",
       "1         1         1         0         0           0         1   0.9167   \n",
       "2         0         1         0         0           1         0   2.0000   \n",
       "3         0         1         0         0           0         1  30.0000   \n",
       "4         0         1         0         0           1         0  25.0000   \n",
       "\n",
       "   sibsp  parch      fare  embarked_C  embarked_Q  embarked_S  \n",
       "0      0      0  211.3375           0           0           1  \n",
       "1      1      2  151.5500           0           0           1  \n",
       "2      1      2  151.5500           0           0           1  \n",
       "3      1      2  151.5500           0           0           1  \n",
       "4      1      2  151.5500           0           0           1  "
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_preprocess_df = pd.read_csv(\n",
    "    'https://github.com/afansi/winterschool18/blob/master/titanic_prepocess.csv?raw=true', \n",
    "    sep=',', \n",
    "    index_col=None\n",
    ")\n",
    "\n",
    "titanic_preprocess_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJcs6PUTavlm"
   },
   "source": [
    "## Découpage en Train / Validation / Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bjbgvffmavlo"
   },
   "source": [
    "Lorsque celà n'a pas déjà été fait,  le dataset est divisé en trois parties:\n",
    "\n",
    "<ol>\n",
    "<li> <b> Train</b> (en général, 60 % du dataset): utilisée pour entraîner le modèle de classification.</li>   \n",
    "<li> <b> Validation</b> (en général, 20 % du dataset): utilisée pour évaluer les performances du modèle en cours d'entraînement.</li>   \n",
    "<li> <b> Test</b> (en général, 20 % du dataset): utilisée pour évaluer les performances de généralisation du modèle entraîné. </li>\n",
    "</ol>\n",
    "\n",
    "Nous utilisons la fonction [np.split](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.split.html) afin de séparer notre jeu de données en sous-ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBmL8VBOavlo"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "train, validate, test = np.split(\n",
    "    titanic_preprocess_df.sample(frac=1, random_state=134), \n",
    "    [int(.6*len(titanic_preprocess_df)), int(.8*len(titanic_preprocess_df))])\n",
    "\n",
    "X_train = train.drop(['survived'], axis=1).values\n",
    "y_train = train['survived'].values\n",
    "\n",
    "X_val = validate.drop(['survived'], axis=1).values\n",
    "y_val = validate['survived'].values\n",
    "\n",
    "X_test = test.drop(['survived'], axis=1).values\n",
    "y_test = test['survived'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wv74TbIWavlr"
   },
   "source": [
    "## Datasets en PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_LJtG-Xavlt"
   },
   "source": [
    "Nous utiliserons la sous-classe <b><a href=\"http://pytorch.org/docs/master/data.html#\"> `torch.utils.data.TensorDataset`</a> </b> qui permet d'encapsuler ensemble les features et la target d'un jeu de données. Nous encapsulerons les données de Train, Validation, et Test définies dans la section précédente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1JtT4tV7avlt"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obEPHnlTavkc"
   },
   "source": [
    "# DEFINITION DU MODELE: Perceptron Multi-couche (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qhN5GL6Gavks"
   },
   "source": [
    "\n",
    "Un perceptron multi-couche est un réseau de neurones à propagation avant. Il prend en entrée les données à traiter, les transforme à travers une série de couches cachées et renvoie une prédiction en sortie.\n",
    "\n",
    "La procédure d'apprentissage typique pour ce modèle consiste en :\n",
    "<ul>\n",
    "<li>Définir l'architecture du réseau. Cela définira les paramètres (non-linéarités, nombre de poids et biais) du réseau.</li>\n",
    "<li>Définir la fonction de coût et l'optimiseur.</li>\n",
    "<li>Entraîner le réseau.</li>\n",
    "<li>Tester le réseau.</li>\n",
    "</ul>\n",
    "\n",
    "Pour résoudre notre tâche, nous allons utiliser un MLP avec les caractéristiques suivantes:\n",
    " <ul>\n",
    " <li> <b> 4 </b> **couches** (<b> 3 </b> couches cachées et <b> 1 </b> couche de sortie) </li>\n",
    " <li> la **dimension** des données d'**entrées** est de <b> 12 . </b></li>\n",
    " <li> les dimensions des différentes **couches intermédiaires** sont <b> 20, 40, 20, 2. </b> </li>\n",
    " <li> utilisation de la fonction d'activation <b> ReLu </b> pour les 3 couches cachées.</li>\n",
    " </ul>\n",
    " \n",
    " Voici un exemple d'architecture que nous allons utiliser: \n",
    " \n",
    "![Alt Text](https://github.com/mila-iqia/ecole_dl_mila_ivado/blob/master/tutoriaux/MLP/images/figures_tuto.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "701t0e-ravkr"
   },
   "source": [
    "## Implémentation du modèle en PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgtCculMavku"
   },
   "source": [
    "### 1 - Boîte à outils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4F5cyijavkv"
   },
   "source": [
    "<ul>\n",
    "<li> La classe <b><a href=\"http://pytorch.org/docs/master/nn.html#module\">`torch.nn.Module`</a></b>: \n",
    "\n",
    "    <br/> En PyTorch, tout réseau de neurones doit <b>hériter</b> de cette classe ou de ses descendantes (sous-classes).\n",
    "    <br/> \n",
    "</li>   \n",
    "<li> La méthode <b>`forward` (...)</b>: \n",
    "    <br/> Toute classe définissant un réseau de neurones doit <b>implémenter</b> la méthode  `forward(...)`. C'est cette méthode qui définit les opérations effectuées par le réseau de neurones et, le cas échéant, construit le graphe computationnel correspondant.\n",
    "    <br/> \n",
    "</li>  \n",
    "<li> La classe <b><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a></b>: \n",
    "    <br/> Cette classe implémente <b>une couche de réseau dense</b> sans fonction d'activation à sa sortie. <br/> Elle prend par défaut deux paramètres: \n",
    "    <ul>\n",
    "    <li><b>`in_features`</b>: la dimension des données en entrée de la couche. </li>\n",
    "    <li><b>`out_features`</b>: la dimension des données en sortie de la couche. </li>    \n",
    "    </ul>\n",
    "    \n",
    "</li>\n",
    "<li> Le module <b><a href=\"http://pytorch.org/docs/master/nn.html#torch-nn-functional\">`torch.nn.functional`</a></b>: \n",
    "<br/> Il définit un ensemble de fonctions qui peuvent être appliquées aux sorties des différentes composantes d'un réseau de neurones. On y retrouve par example:\n",
    "    <ul>\n",
    "    <li> des fonctions non-lineaires: <b><a href=\"http://pytorch.org/docs/master/nn.html#id36\">`sigmoid(...)`</a></b>, <b><a href=\"http://pytorch.org/docs/master/nn.html#id35\">`tanh(...)`</a></b>, <b><a href=\"http://pytorch.org/docs/master/nn.html#id22\">`relu(...)`</a></b>, <a href=\"http://pytorch.org/docs/master/nn.html#non-linear-activation-functions\">`etc...`</a> </li> \n",
    "    <li> des fonctions de coûts: <b><a href=\"http://pytorch.org/docs/master/nn.html#mse-loss\">`mse_loss(...)`</a></b>, <b><a href=\"http://pytorch.org/docs/master/nn.html#nll-loss\">`nll(...)`</a></b>, <b><a href=\"http://pytorch.org/docs/master/nn.html#cross-entropy\">`cross_entropy(...)`</a></b>, <a href=\"http://pytorch.org/docs/master/nn.html#id42\">`etc ...`</a> </li> \n",
    "    <li> des fonctions de régularisation: <b><a href=\"http://pytorch.org/docs/master/nn.html#id38\">`droupout(...)`</a></b>, <a href=\"http://pytorch.org/docs/master/nn.html#dropout-functions\">`etc ...`</a>  </li> \n",
    "    <li> et <a href=\"http://pytorch.org/docs/master/nn.html#torch-nn-functional\">`plein d'autres encore ...`</a> </li> \n",
    "    </ul>\n",
    "    <br/> \n",
    "</li>\n",
    "</ul>\n",
    "\n",
    " les méthodes suivantes sont à compléter :\n",
    "<ul>\n",
    "<li>La méthode `__init__` qui définit les couches. </li>\n",
    "<li>La méthode `forward(input)` qui retourne l'`output`.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMn6-Jepavkw"
   },
   "source": [
    "### 2 - Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NyQGwC-avkw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xR5eBfIbavk0"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(12, 20)\n",
    "        self.fc2 = nn.Linear(20, 40)\n",
    "        self.fc3 = nn.Linear(40, 20)\n",
    "        self.fc4 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvLnHRZ5avk2"
   },
   "source": [
    "## Exécution d'un réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEXgJMDDavk3"
   },
   "source": [
    "\n",
    "Dans cette section, nous exécuterons notre réseau de neurones sur des données aloitoirement générées. \n",
    "\n",
    "### 1 - Boîte à outils\n",
    "<b>Important à savoir:</b>\n",
    "    En PyTorch, il existe deux modes d'exécution d'un réseau de neurones:\n",
    "    <ul>\n",
    "    <li> <b>train</b>: dans ce mode, tous les mécanismes d'apprentissage (construction du graphe computationnel, auto-différentiation) sont mis en place à chaque exécution du réseau. Il est utilisé lorsque le réseau est en cours d'entraînement.</li>\n",
    "    <li> <b>eval</b>: dans ce mode, le modèle est en mode <b>inférence</b>. Il est utilisé lorsque le réseau est en cours d'évaluation.</li>\n",
    "    </ul>\n",
    "<br/>    \n",
    "Nous utiliserons le mode <b>eval</b> dans cette section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpaZwOVENm5q"
   },
   "source": [
    "### 2 - Implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1539777095652,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "gzcABMezavk6",
    "outputId": "0634cf28-4070-42ac-8629-e4d61c2774ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5176, 0.4824],\n",
      "        [0.3129, 0.6871],\n",
      "        [0.3483, 0.6517],\n",
      "        [0.4888, 0.5112],\n",
      "        [0.4927, 0.5073]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiation du réseau\n",
    "neural_net = NeuralNet()\n",
    "neural_net = neural_net.to(device)\n",
    "# activation du mode eval\n",
    "neural_net = neural_net.eval()\n",
    "\n",
    "# Sélection des 5 premières données du dataset de validation\n",
    "data, target = val_dataset[0:5]\n",
    "data = data.to(device)\n",
    "target = target.to(device)\n",
    "# Execution du réseau de neurones\n",
    "output = neural_net(data)   # ou, en version longue, neural_net.forward(data)\n",
    "\n",
    "# Tranformation des resultat en probabilités en utilisant la fonction SOFTMAX\n",
    "output_proba = F.softmax(output, dim=1)\n",
    "\n",
    "# Affichage des probabilités\n",
    "print(output_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVep0BElavlS"
   },
   "source": [
    "Les lignes définissent la sortie du réseau, en terme de <b> probabilités sur deux classes</b>, <b>mort</b> (première colonne) ou <b>survie</b> (deuxième colonne), pour chacune des 5 données en entrée. Prenons le maximum de chaque prédiction comme la prédition de notre modèle et comparons les aux données réelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1539777281397,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "_jV4No36qjdU",
    "outputId": "de321e17-8907-44bc-c487-2d7de8123dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction du modèle\n",
      "tensor([0, 1, 1, 1, 1])\n",
      "Données réelles\n",
      "tensor([0, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Affichage des prédictions (classe ayant la plus grande probabilités)\n",
    "_, prediction = torch.max(output_proba, dim=1)\n",
    "\n",
    "print(\"Prédiction du modèle\")\n",
    "print(prediction)\n",
    "\n",
    "# Affichage de la vrai target\n",
    "print(\"Données réelles\")\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEIIjqOuqjdc"
   },
   "source": [
    "### 3 - Questions: \n",
    "\n",
    "<b> Comment performe notre modèle?</b> <br/>\n",
    "<b> Quelle serait une bonne manière de définir la performance? </b><br/>\n",
    "<b>Comment pouvons-nous améliorer notre modèle?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uySA2TCavmD"
   },
   "source": [
    "## Définir la fonction de coût et l'optimiseur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkoobCLMavmE"
   },
   "source": [
    "## Fonction de coût"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkX7uSXQavmF"
   },
   "source": [
    "La fonction de coût doit être définie en fonction de la tâche que nous souhaitons réaliser.\n",
    "\n",
    "PyTorch offre <a href=\"http://pytorch.org/docs/master/nn.html#id42\">une multitude de fonctions de coûts</a> prêtes à l'emploi.\n",
    "\n",
    "Pour des problèmes de classification, la fonction de coût usuelle est <b> l'entropie croisée (cross-entropy)</b> et c'est elle que nous allons utiliser dans ce tutoriel. En PyTorch, elle est définie par la fonction <b><a href=\"http://pytorch.org/docs/master/nn.html#cross-entropy\">`torch.nn.functional.cross_entropy`</a></b>.  L'entropie croisée est souvent utilisée en optimisation. Elle permet de comparer une distribution $p$ avec une distibution de référence $t$. Elle est minimum lorsque $t=p$. Sa formule pour la calculer entre la prédiction et la cible est : $-\\sum_j t_{ij} \\log(p_{ij})$ où $p$ est la prédiction, $t$ la cible, $i$ les exemples et $j$ les classes de la cible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "47Nvf8IhYn2b"
   },
   "source": [
    "**Implémentation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FHnfYeS5avmF"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def cost_function(prediction, target):\n",
    "    loss = F.cross_entropy(prediction, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vsx_cv9Wqjdj"
   },
   "source": [
    "## Rétro-propagation du gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0hcZaIKtavmH"
   },
   "source": [
    "En Pytorch, grâce au mécanisme de differentiation automatique <a href=\"http://pytorch.org/docs/master/notes/autograd.html\">Autograd</a>, il est possible de calculer automatiquement le gradient de la fonction de coût et de le rétro-propager à travers le graphe computationnel.\n",
    "\n",
    "Pour ce faire, une fois la fonction de coût calculée et stockée dans une variable, il suffit d'appeler la méthode <b> backward() </b> de cette dernière.<br/>\n",
    "<br/>\n",
    "\n",
    "**Snippet de rétro-propagation:**\n",
    "\n",
    "loss = fonction_de_cout(...) <br/>\n",
    "loss.backward()<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YNo_ymYavmH"
   },
   "source": [
    "## Optimiseur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4AlX9TwavmH"
   },
   "source": [
    "PyTorch fournit un <a href=\"http://pytorch.org/docs/master/optim.html#algorithms\">ensemble de méthodes d'optimisation (`torch.optim`)</a> couramment utilisées dans la communauté de Deep Learning. Parmi ces méthodes, on y retrouve notamment: \n",
    "<ul>\n",
    "<li><b>SGD</b> (Stochastic Gradient Descent) <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a> qui est une implémentation de SGD.</li>\n",
    "<li><b>Adam</b> (Adaptive Moment Estimation): variante de la méthode de descente de gradient dans laquelle le taux d'apprentissage est ajusté pour chaque paramètre. Cet ajustement est basé sur le momentum (moyenne glissante des gradients) et la courbure (moyenne glissante de la dérivée seconde). Cet optimiseur a démontré de très bonnes performamces par rapport à SGD dans la litérature.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uam-a0_0qjdl"
   },
   "source": [
    "\n",
    "Pour pouvoir utiliser un optimiseur en PyTorch, il faut l'instancier en lui passant les éléments suivants:\n",
    "<ul>\n",
    "<li><b>Les paramètres du réseau de neurones</b>: ceux-ci s'obtiennent à l'aide de la methode <b>parameters()</b> sur le modèle instanciée.</li>\n",
    "<li><b>Le taux d'apprentissage (learning rate, lr)</b>: c'est le taux d'apprentissage à utiliser pour la mise à jour des paramètres du réseau de neurones pendant le processus d'optimization.</li>\n",
    "<li>Il peut y avoir d'autres paramètres propres à l'optimiseur choisi</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jt6_Qr6ravmI"
   },
   "source": [
    "PyTorch offre un interface simplifiée pour interagir avec tout optimiseur:\n",
    "<ul>\n",
    "<li><b>zero_grad()</b>: Permet d'effacer les gradients des paramètres du réseau de neurones à optimiser. Elle est appelée <b>au début d'une étape d'optimisation</b> afin de re-initialiser les infos sur les paramètres à optimiser. </li>\n",
    "<li><b>step()</b>: Permet d'effectuer une étape d'optimisation. Elle est appelée <b>après une étape de rétro-propagation du gradient</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZ-lKExqavmI"
   },
   "source": [
    "Dans ce tutoriel, nous utiliserons <b>Adam</b> avec une <b>lr</b> de 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WdVIyV9xY2at"
   },
   "source": [
    "**Implémentation: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDMOziJTavmI"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnFOAfdGqjdr"
   },
   "source": [
    "# ENTRAÎNEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YD1paaYCavmJ"
   },
   "source": [
    "## Epoch, Itération, Mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0ksgXqwavmK"
   },
   "source": [
    "### 1- Définition \n",
    "<ol>\n",
    "<li>\n",
    "<b>Epoch</b> : une passe complète sur tout le dataset d'entraînement.\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "<b>Itération</b> : une mise à jour des paramètres du modèle (réseau de neurones). De nombreuses itérations peuvent se produire avant la fin d'un epoch.\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "<b>Mini-batch</b> : Sous-ensemble de données d'entrainement utilisées pour effectuer une mise à jour des paramètres du modèle. Autrement dit, à chaque itération, un mini-batch est utilisé. \n",
    "</li>\n",
    "\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLXjNiDTavmK"
   },
   "source": [
    "### 2 - Mini-batch en PyTorch \n",
    "\n",
    "<ul>\n",
    "<li>\n",
    "PyTorch offre un utilitaire appelé <b><a href=\"http://pytorch.org/docs/master/data.html\"> torch.utils.data.DataLoader </a></b> permettant de charger un dataset quelconque et de le découper automatiquement en mini-batchs.\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6gjERhgavmL"
   },
   "source": [
    "**Bon à savoir**: \n",
    "\n",
    "<ul>\n",
    "<li>\n",
    "Lors de l'entraînement, il est préférable que les données présentées au réseau apparaissent dans <b> un ordre différent d'un epoch à l'autre</b>.\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nsQtU9ylavmL"
   },
   "source": [
    "### 3 - Implémentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7co1OxWJUMpL"
   },
   "source": [
    "Nous allons préparer les `DataLoader` pour nos trois ensembles de données (Entraînement, Validation, et Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGoQZSdqavmM"
   },
   "outputs": [],
   "source": [
    "train_batch_size = 32  # nombre de données dans un batch d'entraînement.\n",
    "eval_batch_size = 32   # nombre de données dans un batch d'évaluation.\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=eval_batch_size, shuffle=False)\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=eval_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ia3ai-GvavmP"
   },
   "source": [
    "## Boucle principale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCxQkQQjwuvq"
   },
   "source": [
    "### 1 - CPU ou GPU\n",
    "\n",
    "**Rappel:** <a href=\"http://pytorch.org/docs/master/cuda.html#module-torch.cuda\">`torch.cuda`</a> est une fonction qui permet la prise en charge de tenseurs de types CUDA avec les mêmes fonctions que les tenseurs de types CPU mais utilisant pour le calcul des GPU. <a href=\"http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available\">`torch.cuda.is_available()`</a> retourne un booléen indiquant si CUDA est présentement disponible. \n",
    "\n",
    "\n",
    "**Conseil:** Definir une variable `device` qui contient le device sur lequel vous souhaitez utiliser pour l'entrainement. Pour passer un tenseur ou un modèle sur le device en question, utiliser la methode `.to(device)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89u5gtjjymwP"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q11ZxpjxzHZm"
   },
   "source": [
    "### 2 - Implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9wNZrTnavmQ"
   },
   "source": [
    "Nous définissons ici notre procédure d'entraînement pour un epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyK9xCsZavmR"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, train_loader, optimizer, device):\n",
    "    \n",
    "    # mettre le modèle en mode train\n",
    "    model.train()\n",
    "    \n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # Accumulateurs d'informations\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # itérer sur les batchs\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # Mettre les données sur le device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # mettre à zéro les gradients des paramètres du réseau de neurones\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # exécuter le réseau de neurones sur les données du batch\n",
    "        prediction = model(data)\n",
    "        \n",
    "        # calculer la fonction de coût par rapport à la target\n",
    "        loss = cost_function(prediction, target)\n",
    "        \n",
    "        # faire la retro-propagation du gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # effectuer un étape d'optimisation\n",
    "        optimizer.step()\n",
    "        \n",
    "        # effectuer la somme des coûts\n",
    "        total_loss += loss.item()*len(data)\n",
    "        \n",
    "        # calculer le nombre de bonnes prédictions (classe correspondante à la valeur maximale en sortie) \n",
    "        _, pred_classes = torch.max(prediction, dim=1)        \n",
    "        correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()\n",
    "        \n",
    "        \n",
    "        \n",
    "    # calculer le coût moyen par epoch\n",
    "    mean_loss = total_loss/len(train_loader.dataset)\n",
    "    \n",
    "    # calculer l'acurracy\n",
    "    acc = correct / len(train_loader.dataset)\n",
    "        \n",
    "    print('Train Epoch: {}   Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
    "        epoch, mean_loss, correct, len(train_loader.dataset),\n",
    "        100. * acc))   \n",
    "    \n",
    "    # retourner le coût moyen obtenu\n",
    "    return mean_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxG666rmavmU"
   },
   "source": [
    "## Procédure d'évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGexbWaHavmU"
   },
   "source": [
    "Nous définissons ici notre procédure d'évaluation du modèle.\n",
    "<br/>\n",
    "En plus de passer le modèle en mode **eval**, il faut penser à désactiver le calcul du gradient (on n'en a pas besoin en mode inférence). <br/>\n",
    "Pour celà, PyTorch offre un ensemble de gestionnaires de contexte permettant de désactiver/activer localement le calcul du gradient:\n",
    "<ol>\n",
    "<li>\n",
    "<a href=\"https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation\">`torch.no_grad()`</a>: désactiver le calcul du gradient.\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "<a href=\"https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation\">`torch.enable_grad()`</a>: activer le calcul du gradient.\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "<a href=\"https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation\">`torch.set_grad_enabled(bool)`</a>: activer/désactiver le calcul du gradient.\n",
    "</li>\n",
    "\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gQj9W5LavmU"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, eval_loader, device):\n",
    "    \n",
    "    # mettre le modèle en mode eval\n",
    "    model.eval()\n",
    "    \n",
    "    # Accumulateurs d'informations\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "    # itérer sur les batchs\n",
    "    for batch_idx, (data, target) in enumerate(eval_loader):\n",
    "\n",
    "        # Mettre les données sur le device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # exécuter le réseau de neurones sur les données du batch\n",
    "        prediction = model(data)\n",
    "\n",
    "        # calculer la fonction de coût par rapport à la target\n",
    "        loss = cost_function(prediction, target)           \n",
    "\n",
    "\n",
    "        # effectuer la somme des coûts\n",
    "        total_loss += loss.item()*len(data)\n",
    "\n",
    "        # calculer le nombre de bonnes prédictions (classe correspondante à la valeur maximale en sortie)\n",
    "        _, pred_classes = torch.max(prediction, dim=1) \n",
    "        correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()         \n",
    "          \n",
    "    \n",
    "    # calculer le coût moyen\n",
    "    mean_loss = total_loss/len(eval_loader.dataset)\n",
    "    \n",
    "    # calculer l'acurracy\n",
    "    acc = correct / len(eval_loader.dataset)\n",
    "        \n",
    "    print('Eval:  Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
    "        mean_loss, correct, len(eval_loader.dataset),\n",
    "        100. * acc)) \n",
    "    \n",
    "    # retourner le coût moyen obtenu\n",
    "    return mean_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMUyZNxdavmW"
   },
   "source": [
    "## Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQLklQXAavmW"
   },
   "source": [
    "Pour des phases d'entraînement qui requièrent beaucoup de temps, il est recommandé de sauvegarder les paramètres (poids) du réseau de neurones au fil de l'apprentissage. C'est ce que l'on appelle communément le <b> checkpointing</b>.\n",
    "\n",
    "PyTorch offre <a href=\"http://pytorch.org/docs/master/notes/serialization.html\">un mécanisme simple</a> pour effectuer cette opération. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ld-Y2gF-avmX"
   },
   "source": [
    "Nous implémentons ici deux méthodes:\n",
    "<ul>\n",
    "<li> la première pour <b> sauvegarder </b> un réseau de neurones </li>\n",
    "<li> la seconde pour <b> charger </b> une sauvegarde de réseau de neurones </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMmNpma2avmX"
   },
   "outputs": [],
   "source": [
    "def save_model(epoch, model, path='./'):\n",
    "    \n",
    "    # creation du nom de fichier indexé par la valeur de l'epoch\n",
    "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
    "    \n",
    "    # sauvegarde des paramètres du modèle.\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZptgqQRavmZ"
   },
   "outputs": [],
   "source": [
    "def load_model(epoch, model, path='./'):\n",
    "    \n",
    "    # creation du nom de fichier indexé par la valeur de l'epoch\n",
    "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
    "    \n",
    "    # chargement des paramètres du modèle sauvegardé.\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ve8sOocWavma"
   },
   "source": [
    "**Important à savoir:**  \n",
    "\n",
    "Il est également possible de sauvegarder <b>l'état de l'optimiseur</b> en PyTorch. Ceci est très important dans les situations où nous souhaitons reprendre l'entraînement du réseau de neurones à partir d'une sauvegarde donnée. Pour plus d'informations, veuillez consulter l'url suivante: https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8lcAP8-1avma"
   },
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6902
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25128,
     "status": "ok",
     "timestamp": 1539780944582,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "keMpyePsavmb",
    "outputId": "25b1ebcc-c800-4077-b9dc-a74b5f89fede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1   Avg_Loss: 0.69287   Acc: 344/625 (55.040%)\n",
      "Eval:  Avg_Loss: 0.66714   Acc: 133/209 (63.636%)\n",
      "Train Epoch: 2   Avg_Loss: 0.65068   Acc: 418/625 (66.880%)\n",
      "Eval:  Avg_Loss: 0.63873   Acc: 137/209 (65.550%)\n",
      "Train Epoch: 3   Avg_Loss: 0.63288   Acc: 419/625 (67.040%)\n",
      "Eval:  Avg_Loss: 0.63616   Acc: 136/209 (65.072%)\n",
      "Train Epoch: 4   Avg_Loss: 0.62606   Acc: 415/625 (66.400%)\n",
      "Eval:  Avg_Loss: 0.61589   Acc: 140/209 (66.986%)\n",
      "Train Epoch: 5   Avg_Loss: 0.61768   Acc: 417/625 (66.720%)\n",
      "Eval:  Avg_Loss: 0.60510   Acc: 139/209 (66.507%)\n",
      "Train Epoch: 6   Avg_Loss: 0.61198   Acc: 427/625 (68.320%)\n",
      "Eval:  Avg_Loss: 0.61334   Acc: 144/209 (68.900%)\n",
      "Train Epoch: 7   Avg_Loss: 0.60842   Acc: 435/625 (69.600%)\n",
      "Eval:  Avg_Loss: 0.60039   Acc: 146/209 (69.856%)\n",
      "Train Epoch: 8   Avg_Loss: 0.60025   Acc: 433/625 (69.280%)\n",
      "Eval:  Avg_Loss: 0.65971   Acc: 121/209 (57.895%)\n",
      "Train Epoch: 9   Avg_Loss: 0.60059   Acc: 422/625 (67.520%)\n",
      "Eval:  Avg_Loss: 0.57870   Acc: 147/209 (70.335%)\n",
      "Train Epoch: 10   Avg_Loss: 0.58589   Acc: 436/625 (69.760%)\n",
      "Eval:  Avg_Loss: 0.60285   Acc: 144/209 (68.900%)\n",
      "Train Epoch: 11   Avg_Loss: 0.57958   Acc: 442/625 (70.720%)\n",
      "Eval:  Avg_Loss: 0.60137   Acc: 141/209 (67.464%)\n",
      "Train Epoch: 12   Avg_Loss: 0.57629   Acc: 448/625 (71.680%)\n",
      "Eval:  Avg_Loss: 0.56043   Acc: 154/209 (73.684%)\n",
      "Train Epoch: 13   Avg_Loss: 0.55922   Acc: 468/625 (74.880%)\n",
      "Eval:  Avg_Loss: 0.55576   Acc: 153/209 (73.206%)\n",
      "Train Epoch: 14   Avg_Loss: 0.55576   Acc: 469/625 (75.040%)\n",
      "Eval:  Avg_Loss: 0.52436   Acc: 154/209 (73.684%)\n",
      "Train Epoch: 15   Avg_Loss: 0.53946   Acc: 468/625 (74.880%)\n",
      "Eval:  Avg_Loss: 0.50660   Acc: 159/209 (76.077%)\n",
      "Train Epoch: 16   Avg_Loss: 0.51513   Acc: 484/625 (77.440%)\n",
      "Eval:  Avg_Loss: 0.48705   Acc: 161/209 (77.033%)\n",
      "Train Epoch: 17   Avg_Loss: 0.50444   Acc: 488/625 (78.080%)\n",
      "Eval:  Avg_Loss: 0.49282   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 18   Avg_Loss: 0.49243   Acc: 498/625 (79.680%)\n",
      "Eval:  Avg_Loss: 0.46451   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 19   Avg_Loss: 0.48690   Acc: 495/625 (79.200%)\n",
      "Eval:  Avg_Loss: 0.46794   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 20   Avg_Loss: 0.47213   Acc: 506/625 (80.960%)\n",
      "Eval:  Avg_Loss: 0.44696   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 21   Avg_Loss: 0.47151   Acc: 500/625 (80.000%)\n",
      "Eval:  Avg_Loss: 0.44775   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 22   Avg_Loss: 0.47764   Acc: 500/625 (80.000%)\n",
      "Eval:  Avg_Loss: 0.44395   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 23   Avg_Loss: 0.46693   Acc: 501/625 (80.160%)\n",
      "Eval:  Avg_Loss: 0.43578   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 24   Avg_Loss: 0.48037   Acc: 483/625 (77.280%)\n",
      "Eval:  Avg_Loss: 0.45011   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 25   Avg_Loss: 0.46109   Acc: 499/625 (79.840%)\n",
      "Eval:  Avg_Loss: 0.45523   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 26   Avg_Loss: 0.45308   Acc: 505/625 (80.800%)\n",
      "Eval:  Avg_Loss: 0.43659   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 27   Avg_Loss: 0.44972   Acc: 499/625 (79.840%)\n",
      "Eval:  Avg_Loss: 0.42789   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 28   Avg_Loss: 0.44528   Acc: 506/625 (80.960%)\n",
      "Eval:  Avg_Loss: 0.42749   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 29   Avg_Loss: 0.49027   Acc: 492/625 (78.720%)\n",
      "Eval:  Avg_Loss: 0.43264   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 30   Avg_Loss: 0.45787   Acc: 504/625 (80.640%)\n",
      "Eval:  Avg_Loss: 0.43155   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 31   Avg_Loss: 0.44184   Acc: 508/625 (81.280%)\n",
      "Eval:  Avg_Loss: 0.42735   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 32   Avg_Loss: 0.44152   Acc: 506/625 (80.960%)\n",
      "Eval:  Avg_Loss: 0.43234   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 33   Avg_Loss: 0.44372   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.43425   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 34   Avg_Loss: 0.43801   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.42469   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 35   Avg_Loss: 0.43998   Acc: 501/625 (80.160%)\n",
      "Eval:  Avg_Loss: 0.41902   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 36   Avg_Loss: 0.44585   Acc: 509/625 (81.440%)\n",
      "Eval:  Avg_Loss: 0.42002   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 37   Avg_Loss: 0.43773   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.41783   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 38   Avg_Loss: 0.43431   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.41807   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 39   Avg_Loss: 0.44402   Acc: 499/625 (79.840%)\n",
      "Eval:  Avg_Loss: 0.41555   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 40   Avg_Loss: 0.45148   Acc: 507/625 (81.120%)\n",
      "Eval:  Avg_Loss: 0.43274   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 41   Avg_Loss: 0.44559   Acc: 505/625 (80.800%)\n",
      "Eval:  Avg_Loss: 0.41605   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 42   Avg_Loss: 0.43735   Acc: 509/625 (81.440%)\n",
      "Eval:  Avg_Loss: 0.43570   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 43   Avg_Loss: 0.43700   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.42842   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 44   Avg_Loss: 0.44193   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.42012   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 45   Avg_Loss: 0.42726   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.48885   Acc: 163/209 (77.990%)\n",
      "Train Epoch: 46   Avg_Loss: 0.45011   Acc: 507/625 (81.120%)\n",
      "Eval:  Avg_Loss: 0.45023   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 47   Avg_Loss: 0.44945   Acc: 505/625 (80.800%)\n",
      "Eval:  Avg_Loss: 0.43610   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 48   Avg_Loss: 0.43844   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.44648   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 49   Avg_Loss: 0.43237   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.41245   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 50   Avg_Loss: 0.42883   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.42045   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 51   Avg_Loss: 0.42844   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.41146   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 52   Avg_Loss: 0.43324   Acc: 508/625 (81.280%)\n",
      "Eval:  Avg_Loss: 0.41553   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 53   Avg_Loss: 0.43502   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.41404   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 54   Avg_Loss: 0.42451   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.41675   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 55   Avg_Loss: 0.42690   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.41558   Acc: 164/209 (78.469%)\n",
      "Train Epoch: 56   Avg_Loss: 0.42573   Acc: 509/625 (81.440%)\n",
      "Eval:  Avg_Loss: 0.41156   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 57   Avg_Loss: 0.42340   Acc: 514/625 (82.240%)\n",
      "Eval:  Avg_Loss: 0.42677   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 58   Avg_Loss: 0.42648   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.42420   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 59   Avg_Loss: 0.42896   Acc: 514/625 (82.240%)\n",
      "Eval:  Avg_Loss: 0.52979   Acc: 160/209 (76.555%)\n",
      "Train Epoch: 60   Avg_Loss: 0.44555   Acc: 499/625 (79.840%)\n",
      "Eval:  Avg_Loss: 0.44141   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 61   Avg_Loss: 0.43008   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.45045   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 62   Avg_Loss: 0.42887   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.43137   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 63   Avg_Loss: 0.43501   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.40921   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 64   Avg_Loss: 0.42841   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.40621   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 65   Avg_Loss: 0.42252   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.41201   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 66   Avg_Loss: 0.44546   Acc: 509/625 (81.440%)\n",
      "Eval:  Avg_Loss: 0.40838   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 67   Avg_Loss: 0.42374   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.40637   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 68   Avg_Loss: 0.42229   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.40491   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 69   Avg_Loss: 0.41940   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.40680   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 70   Avg_Loss: 0.43723   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.41136   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 71   Avg_Loss: 0.45080   Acc: 504/625 (80.640%)\n",
      "Eval:  Avg_Loss: 0.40538   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 72   Avg_Loss: 0.43731   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.41536   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 73   Avg_Loss: 0.42266   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.41748   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 74   Avg_Loss: 0.41739   Acc: 514/625 (82.240%)\n",
      "Eval:  Avg_Loss: 0.41326   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 75   Avg_Loss: 0.41893   Acc: 509/625 (81.440%)\n",
      "Eval:  Avg_Loss: 0.40650   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 76   Avg_Loss: 0.42946   Acc: 509/625 (81.440%)\n",
      "Eval:  Avg_Loss: 0.41027   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 77   Avg_Loss: 0.44247   Acc: 505/625 (80.800%)\n",
      "Eval:  Avg_Loss: 0.46221   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 78   Avg_Loss: 0.42264   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.40775   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 79   Avg_Loss: 0.42612   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.40297   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 80   Avg_Loss: 0.42008   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.40276   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 81   Avg_Loss: 0.41833   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.42682   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 82   Avg_Loss: 0.41896   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.41740   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 83   Avg_Loss: 0.42532   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.40774   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 84   Avg_Loss: 0.43290   Acc: 504/625 (80.640%)\n",
      "Eval:  Avg_Loss: 0.40949   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 85   Avg_Loss: 0.42345   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.40287   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 86   Avg_Loss: 0.42395   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.40247   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 87   Avg_Loss: 0.42150   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.41644   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 88   Avg_Loss: 0.42477   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.42481   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 89   Avg_Loss: 0.42263   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.41217   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 90   Avg_Loss: 0.41863   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.40499   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 91   Avg_Loss: 0.41635   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.39875   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 92   Avg_Loss: 0.41348   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.40343   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 93   Avg_Loss: 0.42197   Acc: 511/625 (81.760%)\n",
      "Eval:  Avg_Loss: 0.40854   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 94   Avg_Loss: 0.41336   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.40000   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 95   Avg_Loss: 0.41138   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.40740   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 96   Avg_Loss: 0.41214   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.39838   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 97   Avg_Loss: 0.41336   Acc: 521/625 (83.360%)\n",
      "Eval:  Avg_Loss: 0.39865   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 98   Avg_Loss: 0.41671   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.40018   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 99   Avg_Loss: 0.41503   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.39782   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 100   Avg_Loss: 0.41200   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.40636   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 101   Avg_Loss: 0.41176   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.39585   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 102   Avg_Loss: 0.41971   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.39738   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 103   Avg_Loss: 0.41258   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.40409   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 104   Avg_Loss: 0.41226   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.40933   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 105   Avg_Loss: 0.41562   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.41088   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 106   Avg_Loss: 0.40967   Acc: 521/625 (83.360%)\n",
      "Eval:  Avg_Loss: 0.39784   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 107   Avg_Loss: 0.41425   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.39883   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 108   Avg_Loss: 0.40988   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.41397   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 109   Avg_Loss: 0.41100   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.40086   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 110   Avg_Loss: 0.41229   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.40019   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 111   Avg_Loss: 0.41252   Acc: 513/625 (82.080%)\n",
      "Eval:  Avg_Loss: 0.40956   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 112   Avg_Loss: 0.41241   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.41808   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 113   Avg_Loss: 0.41485   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.39402   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 114   Avg_Loss: 0.41042   Acc: 510/625 (81.600%)\n",
      "Eval:  Avg_Loss: 0.40605   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 115   Avg_Loss: 0.40934   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.39929   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 116   Avg_Loss: 0.41767   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.43022   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 117   Avg_Loss: 0.41744   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.39875   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 118   Avg_Loss: 0.40933   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.39627   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 119   Avg_Loss: 0.40602   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.42206   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 120   Avg_Loss: 0.41633   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.39626   Acc: 172/209 (82.297%)\n",
      "Train Epoch: 121   Avg_Loss: 0.40839   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.40164   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 122   Avg_Loss: 0.40739   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.41061   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 123   Avg_Loss: 0.40908   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.40003   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 124   Avg_Loss: 0.41115   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.40431   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 125   Avg_Loss: 0.40772   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.42855   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 126   Avg_Loss: 0.41525   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.40469   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 127   Avg_Loss: 0.40687   Acc: 521/625 (83.360%)\n",
      "Eval:  Avg_Loss: 0.39988   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 128   Avg_Loss: 0.40411   Acc: 521/625 (83.360%)\n",
      "Eval:  Avg_Loss: 0.39580   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 129   Avg_Loss: 0.40681   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.39602   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 130   Avg_Loss: 0.40700   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.40452   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 131   Avg_Loss: 0.40213   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.39559   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 132   Avg_Loss: 0.40269   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.39464   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 133   Avg_Loss: 0.40539   Acc: 521/625 (83.360%)\n",
      "Eval:  Avg_Loss: 0.39549   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 134   Avg_Loss: 0.41289   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.39494   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 135   Avg_Loss: 0.41708   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.39914   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 136   Avg_Loss: 0.40617   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.41238   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 137   Avg_Loss: 0.40104   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.39648   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 138   Avg_Loss: 0.40555   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.44476   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 139   Avg_Loss: 0.41186   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.43719   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 140   Avg_Loss: 0.40232   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.39452   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 141   Avg_Loss: 0.41014   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.39873   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 142   Avg_Loss: 0.40107   Acc: 521/625 (83.360%)\n",
      "Eval:  Avg_Loss: 0.41840   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 143   Avg_Loss: 0.40196   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.39370   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 144   Avg_Loss: 0.40331   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.40801   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 145   Avg_Loss: 0.40673   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.42832   Acc: 164/209 (78.469%)\n",
      "Train Epoch: 146   Avg_Loss: 0.40681   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.39848   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 147   Avg_Loss: 0.40166   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.41980   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 148   Avg_Loss: 0.40138   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.51604   Acc: 161/209 (77.033%)\n",
      "Train Epoch: 149   Avg_Loss: 0.48140   Acc: 499/625 (79.840%)\n",
      "Eval:  Avg_Loss: 0.60532   Acc: 154/209 (73.684%)\n",
      "Train Epoch: 150   Avg_Loss: 0.44367   Acc: 500/625 (80.000%)\n",
      "Eval:  Avg_Loss: 0.40582   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 151   Avg_Loss: 0.41967   Acc: 514/625 (82.240%)\n",
      "Eval:  Avg_Loss: 0.40605   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 152   Avg_Loss: 0.40267   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.39810   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 153   Avg_Loss: 0.40174   Acc: 530/625 (84.800%)\n",
      "Eval:  Avg_Loss: 0.39632   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 154   Avg_Loss: 0.40432   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.40466   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 155   Avg_Loss: 0.39873   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.40188   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 156   Avg_Loss: 0.39701   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.41082   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 157   Avg_Loss: 0.39783   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.40689   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 158   Avg_Loss: 0.40105   Acc: 515/625 (82.400%)\n",
      "Eval:  Avg_Loss: 0.40569   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 159   Avg_Loss: 0.39804   Acc: 523/625 (83.680%)\n",
      "Eval:  Avg_Loss: 0.40656   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 160   Avg_Loss: 0.40124   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.39685   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 161   Avg_Loss: 0.39895   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.43889   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 162   Avg_Loss: 0.40820   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.41571   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 163   Avg_Loss: 0.39997   Acc: 512/625 (81.920%)\n",
      "Eval:  Avg_Loss: 0.44514   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 164   Avg_Loss: 0.40346   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.44462   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 165   Avg_Loss: 0.42420   Acc: 517/625 (82.720%)\n",
      "Eval:  Avg_Loss: 0.42703   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 166   Avg_Loss: 0.39925   Acc: 522/625 (83.520%)\n",
      "Eval:  Avg_Loss: 0.40129   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 167   Avg_Loss: 0.39316   Acc: 519/625 (83.040%)\n",
      "Eval:  Avg_Loss: 0.42265   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 168   Avg_Loss: 0.40690   Acc: 520/625 (83.200%)\n",
      "Eval:  Avg_Loss: 0.43253   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 169   Avg_Loss: 0.40154   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.40685   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 170   Avg_Loss: 0.39323   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.40189   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 171   Avg_Loss: 0.40036   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.40016   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 172   Avg_Loss: 0.40910   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.41623   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 173   Avg_Loss: 0.40479   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.40316   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 174   Avg_Loss: 0.39275   Acc: 532/625 (85.120%)\n",
      "Eval:  Avg_Loss: 0.40967   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 175   Avg_Loss: 0.39117   Acc: 529/625 (84.640%)\n",
      "Eval:  Avg_Loss: 0.42012   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 176   Avg_Loss: 0.39282   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.40926   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 177   Avg_Loss: 0.39526   Acc: 516/625 (82.560%)\n",
      "Eval:  Avg_Loss: 0.39570   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 178   Avg_Loss: 0.39885   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.40276   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 179   Avg_Loss: 0.40399   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.41732   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 180   Avg_Loss: 0.38711   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.40007   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 181   Avg_Loss: 0.38893   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.40509   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 182   Avg_Loss: 0.38867   Acc: 525/625 (84.000%)\n",
      "Eval:  Avg_Loss: 0.39993   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 183   Avg_Loss: 0.38702   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.40193   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 184   Avg_Loss: 0.39095   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.39806   Acc: 171/209 (81.818%)\n",
      "Train Epoch: 185   Avg_Loss: 0.38973   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.40874   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 186   Avg_Loss: 0.41039   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.40498   Acc: 170/209 (81.340%)\n",
      "Train Epoch: 187   Avg_Loss: 0.40753   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.40064   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 188   Avg_Loss: 0.38551   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.39840   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 189   Avg_Loss: 0.38561   Acc: 530/625 (84.800%)\n",
      "Eval:  Avg_Loss: 0.43357   Acc: 165/209 (78.947%)\n",
      "Train Epoch: 190   Avg_Loss: 0.40130   Acc: 518/625 (82.880%)\n",
      "Eval:  Avg_Loss: 0.40793   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 191   Avg_Loss: 0.39439   Acc: 529/625 (84.640%)\n",
      "Eval:  Avg_Loss: 0.40367   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 192   Avg_Loss: 0.39617   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.39991   Acc: 166/209 (79.426%)\n",
      "Train Epoch: 193   Avg_Loss: 0.38947   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.40113   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 194   Avg_Loss: 0.39120   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.40935   Acc: 167/209 (79.904%)\n",
      "Train Epoch: 195   Avg_Loss: 0.38511   Acc: 524/625 (83.840%)\n",
      "Eval:  Avg_Loss: 0.40239   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 196   Avg_Loss: 0.38395   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.40431   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 197   Avg_Loss: 0.38497   Acc: 526/625 (84.160%)\n",
      "Eval:  Avg_Loss: 0.40900   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 198   Avg_Loss: 0.38307   Acc: 528/625 (84.480%)\n",
      "Eval:  Avg_Loss: 0.40278   Acc: 168/209 (80.383%)\n",
      "Train Epoch: 199   Avg_Loss: 0.38622   Acc: 527/625 (84.320%)\n",
      "Eval:  Avg_Loss: 0.41380   Acc: 169/209 (80.861%)\n",
      "Train Epoch: 200   Avg_Loss: 0.38013   Acc: 530/625 (84.800%)\n",
      "Eval:  Avg_Loss: 0.39723   Acc: 166/209 (79.426%)\n",
      "\n",
      "\n",
      "\n",
      "Optimization ended.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nombre d'epochs\n",
    "numEpochs = 200\n",
    "\n",
    "# Frequence de sauvegarde\n",
    "checkpoint_freq = 10\n",
    "\n",
    "# Repertoire pour la sauvegarde des données\n",
    "path = './'\n",
    "\n",
    "# Accumulateurs des coûts moyens obtenu par epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Accumulateurs des performances par epoch\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Définition du réseau de neuronnes\n",
    "neural_net = NeuralNet()\n",
    "\n",
    "\n",
    "# Mettre le réseau sur le device\n",
    "neural_net = neural_net.to(device)\n",
    "\n",
    "# définition de l'optimiseur\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) \n",
    "# optimizer = optim.SGD(neural_net.parameters(), lr=0.001) \n",
    "\n",
    "\n",
    "# Itérer sur le nombre d'epochs\n",
    "for epoch in range(1, numEpochs + 1):\n",
    "    \n",
    "    # entraîner le modèle avec le dataset de train\n",
    "    train_loss, train_acc = train(epoch, neural_net, train_loader, optimizer, device)   \n",
    "    \n",
    "    # évaluer le modèle avec le dataset de validation\n",
    "    val_loss, val_acc = evaluate(neural_net, val_loader, device)       \n",
    "    \n",
    "    # Sauvegarde des coûts obtenus\n",
    "    train_losses.append(train_loss)    \n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Sauvegarde des performamces\n",
    "    train_accuracies.append(train_acc)    \n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Checkpoint\n",
    "    if epoch % checkpoint_freq ==0:\n",
    "        save_model(epoch, neural_net, path)\n",
    "\n",
    "# Sauvegarde du modèle à la fin de l'entraînement.\n",
    "save_model(numEpochs, neural_net, path)\n",
    "    \n",
    "print(\"\\n\\n\\nOptimization ended.\\n\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86OZRLrjavmd"
   },
   "source": [
    "## Exécution du réseau de neurones avec des données réelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1539781342192,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "mklvQruYavme",
    "outputId": "fe3c24b4-b92e-42d8-86c7-599e0276b724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8423, 0.1577],\n",
      "        [0.9622, 0.0378],\n",
      "        [0.5178, 0.4822],\n",
      "        [0.7668, 0.2332],\n",
      "        [0.1343, 0.8657],\n",
      "        [0.1159, 0.8841],\n",
      "        [0.4826, 0.5174],\n",
      "        [0.0515, 0.9485],\n",
      "        [0.5029, 0.4971],\n",
      "        [0.2109, 0.7891]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# activation du mode eval\n",
    "neural_net = neural_net.eval()\n",
    "\n",
    "# Sélection des 10 premières données du dataset de validation\n",
    "data, target = val_dataset[0:10]\n",
    "data = data.to(device)\n",
    "# Execution du réseau de neurones\n",
    "output = neural_net(data)   # ou, en version longue, neural_net.forward(data)\n",
    "\n",
    "# Tranformation des resultat en probabilités en utilisant la fonction SOFTMAX\n",
    "output_proba = F.softmax(output, dim=1)\n",
    "\n",
    "# Affichage des probabilités\n",
    "print(output_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 230,
     "status": "ok",
     "timestamp": 1539781343276,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "RvIEqKt0qjeT",
    "outputId": "fcb08628-9247-4d8c-8a20-1282dfc2a79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction du modèle\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "Données réelles\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Affichage des prédictions (classe ayant la plus grande probabilités)\n",
    "_, prediction = torch.max(output_proba, dim=1)\n",
    "\n",
    "print(\"Prédiction du modèle\")\n",
    "print(prediction)\n",
    "\n",
    "# Affichage de la vrai target\n",
    "print(\"Données réelles\")\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V11J3Jihavmy"
   },
   "source": [
    "## Visualisation de la courbe d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9_9C_tXavmz"
   },
   "source": [
    "La <b>visualisation de la courbe d'apprentissage</b> permet de détecter d'éventuels problèmes survenus lors de l'apprentissage, par exemple, l'overfitting (sur-apprentissage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1539781147825,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "iNcbpl0tavm0",
    "outputId": "cd28d35a-eaa2-4a68-fa98-72d7124bbb72"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEHCAYAAABV4gY/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVEOUclcoFIpqyP8B1t9eE+nNtXkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f967d90a278>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = list(range(len(train_losses)))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(x, train_losses, 'r', label=\"Train\")\n",
    "plt.plot(x, val_losses, 'g', label=\"Validation\")\n",
    "plt.title('Loss')\n",
    "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=True, fancybox=True)\n",
    "leg.get_frame().set_alpha(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1539781152991,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "g-VGQ2pMavm4",
    "outputId": "fc84f290-8c6b-456e-8ba9-fa53f405dc0e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEHCAYAAABV4gY/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9633f5b780>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(len(train_accuracies)))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(x, train_accuracies, 'r', label=\"Train\")\n",
    "plt.plot(x, val_accuracies, 'g', label=\"Validation\")\n",
    "plt.title('Accuracy')\n",
    "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
    "leg.get_frame().set_alpha(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8oktkpkuqjet"
   },
   "source": [
    "**Questions: **\n",
    "\n",
    "<b> a) Que pouvez-vous dire de ces courbes ?  <br/>\n",
    "Illustrent-ils un régime de sur-apprentissage? Si non, pourquoi? Si oui, que pouvez-vous faire pour y remédier?</b> <br/>\n",
    "<b> b) Que pouvez-vous faire pour améliorer la performance du réseau de neurones sur des données de validation? </b><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XK_eUsq3avm8"
   },
   "source": [
    "# ÉVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4UREO5elavm8"
   },
   "source": [
    "Nous pouvons finalement évaluer notre modèle apppris sur notre dataset de Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1539781169688,
     "user": {
      "displayName": "Jeremy Pinto",
      "photoUrl": "",
      "userId": "01769873812237395022"
     },
     "user_tz": 240
    },
    "id": "pPWvDM-qavm8",
    "outputId": "1a81293b-75e5-4b3b-b61d-17518a6e9017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval:  Avg_Loss: 0.51039   Acc: 167/209 (79.904%)\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(neural_net, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvP_-KUwqjez"
   },
   "source": [
    "**Questions: **\n",
    "\n",
    "<b> a) Comparer les résultats de validation et de test ?  Le réseau appris généralise t'il aussi bien qu'espéré ? <br/>\n",
    "b) Pensez-vous qu'il est possible d'utiliser un MLP pour d'autres types de données comme des image par exemple? </b> <br/>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mgtCculMavku"
   ],
   "name": "MLP-AFT-Hiver18Tutorial-Complet.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
